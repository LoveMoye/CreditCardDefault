{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna를 이용해서 parameter tuning하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Module\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Variable\n",
    "\n",
    "random_state = 475\n",
    "\n",
    "info = [\"LIMIT_BAL\",\"SEX\",\"EDUCATION\",\"MARRIAGE\",\"AGE\"]\n",
    "delay_n = [\"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\"]\n",
    "bill_n = [\"BILL_AMT1\",\"BILL_AMT2\",\"BILL_AMT3\",\"BILL_AMT4\",\"BILL_AMT5\",\"BILL_AMT6\"]\n",
    "pay_n = [\"PAY_AMT1\",\"PAY_AMT2\",\"PAY_AMT3\",\"PAY_AMT4\",\"PAY_AMT5\",\"PAY_AMT6\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"C:/Users/chowonjae/Desktop/내부 프로젝트/uci_creditcard-train-0.0-0.0 (1).csv\")\n",
    "test = pd.read_csv(\"C:/Users/chowonjae/Desktop/내부 프로젝트/uci_creditcard-test-0.0-0.0 (1).csv\")\n",
    "\n",
    "length = len(train)\n",
    "\n",
    "#Change the column name \"default payment next month\" -> \"default\"\n",
    "train = train.rename(columns = {\"default payment next month\":\"default\"})\n",
    "test = test.rename(columns = {\"default payment next month\":\"default\"})\n",
    "\n",
    "train = train.drop([\"ID\",\"sep_idx\"], axis = 1)\n",
    "train_drop_info = train.drop(info, axis = 1)\n",
    "\n",
    "test = test.drop([\"ID\",\"sep_idx\"], axis = 1)\n",
    "\n",
    "# Change Type\n",
    "\n",
    "train[\"SEX\"] = train[\"SEX\"].astype(np.int)\n",
    "train[\"EDUCATION\"] = train[\"EDUCATION\"].astype(np.int)\n",
    "train[\"MARRIAGE\"] = train[\"MARRIAGE\"].astype(np.int)\n",
    "train[\"AGE\"] = train[\"AGE\"].astype(np.int)\n",
    "train[\"default\"] = train[\"default\"].astype(np.int)\n",
    "train[delay_n] = train[delay_n].astype(np.int)\n",
    "\n",
    "test[\"SEX\"] = test[\"SEX\"].astype(np.int)\n",
    "test[\"EDUCATION\"] = test[\"EDUCATION\"].astype(np.int)\n",
    "test[\"MARRIAGE\"] = test[\"MARRIAGE\"].astype(np.int)\n",
    "test[\"AGE\"] = test[\"AGE\"].astype(np.int)\n",
    "test[\"default\"] = test[\"default\"].astype(np.int)\n",
    "test[delay_n] = test[delay_n].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(\"default\", axis = 1)\n",
    "y_train = train[\"default\"]\n",
    "\n",
    "X_test = test.drop(\"default\", axis = 1)\n",
    "y_test = test[\"default\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    \n",
    "    true_positive_mask = np.logical_and((y_true == 1), (y_pred == 1))\n",
    "    true_negative_mask = np.logical_and((y_true == 0), (y_pred == 0))\n",
    "    condition_positive = (y_true == 1)\n",
    "    predicted_positive = (y_pred == 1)\n",
    "    \n",
    "    precision = np.sum(true_positive_mask) / np.sum(predicted_positive)\n",
    "    recall = np.sum(true_positive_mask) / np.sum(condition_positive)\n",
    "    \n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    acc = (np.sum(true_positive_mask) + np.sum(true_negative_mask)) / len(y_true)\n",
    "    \n",
    "    return precision, recall, f1, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "apr = [\"PAY_6\", \"BILL_AMT6\", \"PAY_AMT6\"]\n",
    "may = [\"PAY_5\", \"BILL_AMT5\", \"PAY_AMT5\"]\n",
    "jun = [\"PAY_4\", \"BILL_AMT4\", \"PAY_AMT4\"]\n",
    "jul = [\"PAY_3\", \"BILL_AMT3\", \"PAY_AMT3\"]\n",
    "aug = [\"PAY_2\", \"BILL_AMT2\", \"PAY_AMT2\"]\n",
    "sep = [\"PAY_0\", \"BILL_AMT1\", \"PAY_AMT1\"]\n",
    "month_list = [apr, may, jun, jul, aug, sep]\n",
    "\n",
    "def LSTM_Input(df):\n",
    "    \n",
    "    total = len(df) * 6 * 3 # 숫자 총 갯수\n",
    "    df_preprocessing = np.array([.0] * total).reshape(-1, 6 ,3) # 처리한 데이터 저장할 array\n",
    "    df_unit = np.array([.0] * (18)).reshape(6, 3)\n",
    "    for i in range(len(df)):\n",
    "        X_df_i = df.loc[i]\n",
    "        for j in range(6):\n",
    "            df_unit[j] = df.loc[i][month_list[j]].values\n",
    "        df_preprocessing[i] = df_unit\n",
    "    \n",
    "    return df_preprocessing\n",
    "\n",
    "def preprocessing(df, test = False):\n",
    "    \n",
    "    if not test:\n",
    "        y = df[\"default\"]\n",
    "\n",
    "        one_hot = np.unique([0, 1]).shape[0]\n",
    "        y_preprocessing = np.eye(one_hot)[y.to_numpy()].reshape(-1, 2)\n",
    "\n",
    "    ##MinMaxScaling\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    mms = MinMaxScaler()\n",
    "    mms.fit(df[[\"LIMIT_BAL\"] + bill_n + pay_n])\n",
    "\n",
    "    X_scaling = mms.transform(df[[\"LIMIT_BAL\"] + bill_n + pay_n])\n",
    "\n",
    "    X_scaling_df = pd.DataFrame(X_scaling, columns = [\"LIMIT_BAL\"] + bill_n + pay_n)\n",
    "\n",
    "    for delay in delay_n:    \n",
    "        X_scaling_df[delay] = df[delay].reset_index(drop = True)\n",
    "\n",
    "    X_add_feature = df[info]\n",
    "    X_add_feature.loc[:,\"LIMIT_BAL\"] = X_scaling_df[\"LIMIT_BAL\"].copy()\n",
    "    X_add_feature = X_add_feature.to_numpy()\n",
    "\n",
    "    X_preprocessing = LSTM_Input(X_scaling_df)\n",
    "    \n",
    "    if test:\n",
    "        return X_preprocessing, X_add_feature\n",
    "    \n",
    "    y = df[\"default\"]\n",
    "\n",
    "    one_hot = np.unique([0, 1]).shape[0]\n",
    "    y_preprocessing = np.eye(one_hot)[y.to_numpy()].reshape(-1, 2)\n",
    "        \n",
    "    return X_preprocessing, X_add_feature, y, y_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sampling(train, proportion, size = None, return_default = False):\n",
    "    \n",
    "    if return_default:\n",
    "        return train\n",
    "    \n",
    "    if isinstance(size, int) or isinstance(size, str):\n",
    "        \n",
    "        if size == \"same\":\n",
    "            size = train[train['default'] == 0].shape[0]\n",
    "        \n",
    "        succeed_sample = train[train['default'] == 0].shape[0]\n",
    "        succeed = train[train['default'] == 0].reset_index(drop = True)\n",
    "\n",
    "        default_sample = train[train['default'] == 1].shape[0]\n",
    "        idx = np.random.randint(0, default_sample, size)\n",
    "        default = train[train['default'] == 1].reset_index(drop = True).iloc[idx]\n",
    "        \n",
    "        df = pd.concat([succeed, default])\n",
    "        \n",
    "        return df.reset_index(drop = True)\n",
    "    \n",
    "    \n",
    "    succeed_sample = train[train['default'] == 0].shape[0]\n",
    "    succeed = train[train['default'] == 0].reset_index(drop = True)\n",
    "\n",
    "    default_sample = train[train['default'] == 1].shape[0]\n",
    "    idx = np.random.randint(0, default_sample, int(len(train) * proportion))\n",
    "    default = train[train['default'] == 1].reset_index(drop = True).iloc[idx]\n",
    "\n",
    "    df = pd.concat([succeed, default])\n",
    "    \n",
    "    return df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\py37tf20\\lib\\site-packages\\pandas\\core\\indexing.py:1745: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n",
      "C:\\Anaconda\\envs\\py37tf20\\lib\\site-packages\\pandas\\core\\indexing.py:1745: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(train, test_size = 0.2, stratify = y_train)\n",
    "train_df = train_df.reset_index(drop = True)\n",
    "val_df = val_df.reset_index(drop = True)\n",
    "\n",
    "X_train_preprocessing, X_train_add_feature, y_train_before, y_train_preprocessing = preprocessing(train_df)\n",
    "X_val_preprocessing, X_val_add_feature, y_val_before, y_val_preprocessing = preprocessing(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = [X_train_preprocessing, X_train_add_feature, y_train_before, y_train_preprocessing]\n",
    "val_list = [X_val_preprocessing, X_val_add_feature, y_val_before, y_val_preprocessing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/원영/Habit_project/train_test_split/train.pkl', 'wb') as f:\n",
    "    pickle.dump(train_list, f)\n",
    "    \n",
    "with open('D:/원영/Habit_project/train_test_split/val.pkl', 'wb') as f:\n",
    "    pickle.dump(val_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    with open('D:/원영/Habit_project/train_test_split/train.pkl', 'rb') as f:\n",
    "        train_list = pickle.load(f)\n",
    "        \n",
    "    with open('D:/원영/Habit_project/train_test_split/val.pkl', 'rb') as f:\n",
    "        val_list = pickle.load(f)\n",
    "    \n",
    "    return train_list, val_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_skf_data():\n",
    "    \n",
    "    with open('D:/원영/Habit_project/Sampling_pkl/skf_sampling_train_list.pkl', 'rb') as f:\n",
    "        skf_oversam_train_list = pickle.load(f)\n",
    "        \n",
    "    with open('D:/원영/Habit_project/Sampling_pkl/skf_sampling_val_list.pkl', 'rb') as f:\n",
    "        skf_oversam_val_list = pickle.load(f)\n",
    "        \n",
    "    for i in range(5):\n",
    "        skf_oversam_val_list[i][0][1] =  skf_oversam_val_list[i][0][1].to_numpy()\n",
    "        for j in range(6):\n",
    "            skf_oversam_train_list[i][j][1] =  skf_oversam_train_list[i][j][1].to_numpy()\n",
    "    \n",
    "    return skf_oversam_train_list, skf_oversam_val_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (6,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = 2\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For tuning hyperparameter\n",
    "\n",
    "def optunaModel(trial):\n",
    "    # Input layer\n",
    "    input_layer = layers.Input(shape = shape, name = \"input1\")\n",
    "    add_feature = layers.Input(shape = (5), name = \"input2\")\n",
    "    \n",
    "    #n_layers = trial.suggest_int(\"n_layers\", 0, 1)\n",
    "    n_layers = 0\n",
    "    if n_layers == 0:\n",
    "        lstm = layers.LSTM(3)(input_layer)\n",
    "    else:\n",
    "        lstm = layers.LSTM(3, return_sequences = True)(input_layer)\n",
    "        for i in range(n_layers):\n",
    "            if i == n_layers - 1:\n",
    "                lstm = layers.LSTM(3)(lstm)\n",
    "            else:\n",
    "                lstm = layers.LSTM(3, return_sequences = True)(lstm)\n",
    "\n",
    "    # Add features\n",
    "    add = layers.Concatenate()([lstm, add_feature])\n",
    "    \n",
    "    num_hidden = trial.suggest_int(\"n_units\", 64, 512, log=True)\n",
    "    num_rate = trial.suggest_int(\"dropout_rate\", 3, 5)\n",
    "    # output layer\n",
    "    fc1 = layers.Dense(num_hidden, activation = \"relu\")(add)\n",
    "    do1 = layers.Dropout(num_rate * 0.1)(fc1)\n",
    "    fc2 = layers.Dense(num_hidden, activation = \"relu\")(do1)\n",
    "    do2 = layers.Dropout(num_rate * 0.1)(fc2)\n",
    "    output = layers.Dense(CLASSES, activation = 'sigmoid')(do2)  ## linear output\n",
    "    \n",
    "    model = keras.Model(inputs = [input_layer, add_feature], outputs = output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_optimizer(trial):\n",
    "    # We optimize the choice of optimizers as well as their parameters.\n",
    "    kwargs = {}\n",
    "    optimizer_options = [\"Adam\"]\n",
    "    optimizer_selected = trial.suggest_categorical(\"optimizer\", optimizer_options)\n",
    "    if optimizer_selected == \"Adam\":\n",
    "        kwargs[\"learning_rate\"] = trial.suggest_float(\"adam_learning_rate\", 1e-5, 1e-1, log=True)\n",
    "\n",
    "    optimizer = getattr(tf.optimizers, optimizer_selected)(**kwargs)\n",
    "    return optimizer\n",
    "\n",
    "def learn(model, optimizer, dataset, mode=\"eval\"):\n",
    "    binary_crossentropy = tf.metrics.BinaryCrossentropy(\"binary_crossentropy'\", dtype=tf.float32)\n",
    "\n",
    "    for batch, (features, labels) in enumerate(dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(features, training=(mode == \"train\"))\n",
    "            loss_value = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(labels, y_pred, from_logits=False))\n",
    "            if mode == \"eval\":\n",
    "                binary_crossentropy(labels, y_pred)\n",
    "            else:\n",
    "                grads = tape.gradient(loss_value, model.variables)\n",
    "                optimizer.apply_gradients(zip(grads, model.variables))\n",
    "\n",
    "    if mode == \"eval\":\n",
    "        return binary_crossentropy\n",
    "    \n",
    "    \n",
    "def get_data(trial):\n",
    "    train_list, val_list = load_skf_data()\n",
    "    \n",
    "    skf_train_list_1 = train_list[0][0]\n",
    "    skf_val_list_1 = val_list[0][0]\n",
    "    \n",
    "    BATCHSIZE = trial.suggest_int(\"batchsize\", 32, 256,  log=True)\n",
    "    \n",
    "    train_ds = tf.data.Dataset.from_tensor_slices(({'input1': skf_train_list_1[0], 'input2': skf_train_list_1[1]}, skf_train_list_1[3]))\n",
    "    train_ds = train_ds.shuffle(len(skf_train_list_1[0])).batch(BATCHSIZE)\n",
    "\n",
    "    valid_ds = tf.data.Dataset.from_tensor_slices(({'input1': skf_val_list_1[0], 'input2': skf_val_list_1[1]}, skf_val_list_1[3]))\n",
    "    valid_ds = valid_ds.shuffle(len(skf_val_list_1[0])).batch(BATCHSIZE)\n",
    "    return train_ds, valid_ds\n",
    "\n",
    "def objective(trial):\n",
    "    # Get data.\n",
    "    train_ds, valid_ds = get_data(trial)\n",
    "\n",
    "    # Build model and optimizer.\n",
    "    model = optunaModel(trial)\n",
    "    optimizer = create_optimizer(trial)\n",
    "\n",
    "    # Training and validating cycle.\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        for _ in range(EPOCHS):\n",
    "            learn(model, optimizer, train_ds, \"train\")\n",
    "\n",
    "        binary_crossentropy = learn(model, optimizer, valid_ds, \"eval\")\n",
    "\n",
    "    # Return last validation accuracy.\n",
    "    return binary_crossentropy.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-11-23 13:15:26,203]\u001b[0m A new study created in memory with name: no-name-604f6adc-b7b9-4e7b-8eaf-7dfae52a2038\u001b[0m\n",
      "\u001b[32m[I 2020-11-23 13:31:12,039]\u001b[0m Trial 0 finished with value: 7.666619300842285 and parameters: {'batchsize': 62, 'n_units': 375, 'dropout_rate': 5, 'optimizer': 'Adam', 'adam_learning_rate': 0.045779610539576604}. Best is trial 0 with value: 7.666619300842285.\u001b[0m\n",
      "\u001b[32m[I 2020-11-23 13:45:49,289]\u001b[0m Trial 1 finished with value: 7.666621208190918 and parameters: {'batchsize': 58, 'n_units': 261, 'dropout_rate': 5, 'optimizer': 'Adam', 'adam_learning_rate': 0.0007419912844442669}. Best is trial 0 with value: 7.666619300842285.\u001b[0m\n",
      "\u001b[32m[I 2020-11-23 13:55:43,939]\u001b[0m Trial 2 finished with value: 0.5376139283180237 and parameters: {'batchsize': 91, 'n_units': 130, 'dropout_rate': 3, 'optimizer': 'Adam', 'adam_learning_rate': 9.863877842868509e-05}. Best is trial 2 with value: 0.5376139283180237.\u001b[0m\n",
      "\u001b[32m[I 2020-11-23 13:59:54,025]\u001b[0m Trial 3 finished with value: 5.507697105407715 and parameters: {'batchsize': 227, 'n_units': 190, 'dropout_rate': 5, 'optimizer': 'Adam', 'adam_learning_rate': 1.2998603134574089e-05}. Best is trial 2 with value: 0.5376139283180237.\u001b[0m\n",
      "\u001b[32m[I 2020-11-23 14:28:33,205]\u001b[0m Trial 4 finished with value: 0.5109953880310059 and parameters: {'batchsize': 33, 'n_units': 235, 'dropout_rate': 3, 'optimizer': 'Adam', 'adam_learning_rate': 8.119477310627347e-05}. Best is trial 4 with value: 0.5109953880310059.\u001b[0m\n",
      "\u001b[32m[I 2020-11-23 14:38:39,271]\u001b[0m Trial 5 finished with value: 7.666621208190918 and parameters: {'batchsize': 106, 'n_units': 147, 'dropout_rate': 5, 'optimizer': 'Adam', 'adam_learning_rate': 0.013178377080915593}. Best is trial 4 with value: 0.5109953880310059.\u001b[0m\n",
      "\u001b[32m[I 2020-11-23 14:55:51,156]\u001b[0m Trial 6 finished with value: 0.5570841431617737 and parameters: {'batchsize': 60, 'n_units': 122, 'dropout_rate': 3, 'optimizer': 'Adam', 'adam_learning_rate': 8.323133785535498e-05}. Best is trial 4 with value: 0.5109953880310059.\u001b[0m\n",
      "\u001b[32m[I 2020-11-23 15:20:35,294]\u001b[0m Trial 7 finished with value: 7.666614532470703 and parameters: {'batchsize': 50, 'n_units': 85, 'dropout_rate': 3, 'optimizer': 'Adam', 'adam_learning_rate': 0.059011011669094365}. Best is trial 4 with value: 0.5109953880310059.\u001b[0m\n",
      "\u001b[32m[I 2020-11-23 15:30:14,798]\u001b[0m Trial 8 finished with value: 7.6666154861450195 and parameters: {'batchsize': 144, 'n_units': 248, 'dropout_rate': 5, 'optimizer': 'Adam', 'adam_learning_rate': 0.009939459971757209}. Best is trial 4 with value: 0.5109953880310059.\u001b[0m\n",
      "\u001b[32m[I 2020-11-23 15:50:20,916]\u001b[0m Trial 9 finished with value: 7.666624546051025 and parameters: {'batchsize': 64, 'n_units': 257, 'dropout_rate': 5, 'optimizer': 'Adam', 'adam_learning_rate': 0.01840586370236994}. Best is trial 4 with value: 0.5109953880310059.\u001b[0m\n",
      "\u001b[32m[I 2020-11-23 16:29:49,409]\u001b[0m Trial 10 finished with value: 0.5234672427177429 and parameters: {'batchsize': 34, 'n_units': 499, 'dropout_rate': 4, 'optimizer': 'Adam', 'adam_learning_rate': 1.436560296887704e-05}. Best is trial 4 with value: 0.5109953880310059.\u001b[0m\n",
      "\u001b[32m[I 2020-11-23 17:12:47,569]\u001b[0m Trial 11 finished with value: 0.5248963832855225 and parameters: {'batchsize': 32, 'n_units': 463, 'dropout_rate': 4, 'optimizer': 'Adam', 'adam_learning_rate': 2.733366378005691e-05}. Best is trial 4 with value: 0.5109953880310059.\u001b[0m\n",
      "\u001b[32m[I 2020-11-23 17:55:21,259]\u001b[0m Trial 12 finished with value: 7.666625022888184 and parameters: {'batchsize': 32, 'n_units': 480, 'dropout_rate': 4, 'optimizer': 'Adam', 'adam_learning_rate': 0.0004072344380339353}. Best is trial 4 with value: 0.5109953880310059.\u001b[0m\n",
      "\u001b[32m[I 2020-11-23 18:28:59,139]\u001b[0m Trial 13 finished with value: 0.5373625159263611 and parameters: {'batchsize': 40, 'n_units': 329, 'dropout_rate': 4, 'optimizer': 'Adam', 'adam_learning_rate': 1.0416410298861406e-05}. Best is trial 4 with value: 0.5109953880310059.\u001b[0m\n",
      "\u001b[32m[I 2020-11-23 19:03:25,722]\u001b[0m Trial 14 finished with value: 2.060170888900757 and parameters: {'batchsize': 39, 'n_units': 83, 'dropout_rate': 4, 'optimizer': 'Adam', 'adam_learning_rate': 8.036127569324909e-05}. Best is trial 4 with value: 0.5109953880310059.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  15\n",
      "Best trial:\n",
      "  Value:  0.5109953880310059\n",
      "  Params: \n",
      "    batchsize: 33\n",
      "    n_units: 235\n",
      "    dropout_rate: 3\n",
      "    optimizer: Adam\n",
      "    adam_learning_rate: 8.119477310627347e-05\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials = 15)\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37tf20]",
   "language": "python",
   "name": "conda-env-py37tf20-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
